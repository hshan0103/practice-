{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hshan/opt/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from  datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plot \n",
    "import sklearn\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(prev_last, pred_period, is_train=True, store='CA_1'):\n",
    "    \n",
    "    cal_cat_cols = ['weekday', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    sales_cat_cols = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "    price_cat_cols = ['store_id', 'item_id']\n",
    "    \n",
    "    calendars = pd.read_csv('/Users/hshan/Downloads/M5/calendar.csv')\n",
    "    sales = pd.read_csv('/Users/hshan/Downloads/M5/sales_train_validation.csv')\n",
    "    prices = pd.read_csv('/Users/hshan/Downloads/M5/sell_prices.csv')\n",
    "    \n",
    "    if not is_train:\n",
    "        for i in range((prev_last+1), (prev_last+pred_period+1)):\n",
    "            f_string = f'd_{i}'\n",
    "            sales[f_string] = pd.Series()\n",
    "    \n",
    "    ind_var = ['id'] + sales_cat_cols\n",
    "    val_var = [col for col in sales.columns if col.startswith('d_')]\n",
    "    df = pd.melt(sales, id_vars = ind_var, value_vars = val_var, var_name = 'd', value_name='sales')\n",
    "    df = df.merge(calendars, on = 'd', copy = False)\n",
    "    df = df.merge(prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n",
    "    \n",
    "    df = df[df['store_id']==store]\n",
    "    \n",
    "    cat_cols = cal_cat_cols + sales_cat_cols\n",
    "    label_encoder = LabelEncoder()\n",
    "    for col in cat_cols:\n",
    "        df[col] = df[col].fillna('').astype('category')\n",
    "        df[col] = label_encoder.fit_transform(df[col])\n",
    "\n",
    "    unused_cols = ['wm_yr_wk', 'weekday','store_id']\n",
    "    \n",
    "    df.drop(unused_cols, inplace = True, axis = 1)\n",
    "    \n",
    "    return (df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_features(df):\n",
    "    '''max lag should not be exceeding 57 in this case'''\n",
    "    num = [1,7,28]\n",
    "    lags = num\n",
    "    windows = num\n",
    "    lag_cols = [f'lag_{lag}' for lag in lags]\n",
    "    \n",
    "\n",
    "    for lag, lag_col in zip(lags, lag_cols):\n",
    "        df[lag_col] = df[['id','sales']].groupby('id')['sales'].shift(lag)\n",
    "        \n",
    "    for window in windows:\n",
    "        for lag, lag_col in zip(lags, lag_cols):\n",
    "            mean_col = f'mean_{lag}_{window}'\n",
    "            df[mean_col] = df[['id',lag_col]].groupby('id')[lag_col].transform(lambda x: x.rolling(window).mean())\n",
    "    \n",
    "    return (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(x_features_df, n_components):\n",
    "    pca = PCA(n_components)\n",
    "    pca_cols = []\n",
    "    for i in range(n_components):\n",
    "        pca_col = f'pc_{i+1}'\n",
    "        pca_cols.append(pca_col)\n",
    "    p_components = pca.fit_transform(x_features_df)\n",
    "    pca_features_df = pd.DataFrame(data = p_components, columns = pca_cols)\n",
    "    \n",
    "    return(pca_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_period = 56\n",
    "train_start = 1\n",
    "train_last = 1913\n",
    "test_start = train_last + 1\n",
    "test_last  = train_last + pred_period\n",
    "pred_start = test_last + 1\n",
    "pred_last = test_last + pred_period\n",
    "seed = 1231\n",
    "\n",
    "removed_cols = ['id', 'date', 'sales', 'd', 'wm_yr_wk', 'weekday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_df(train_last, pred_period, is_train=True, store='WI_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lag_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "y_target = df['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols = list(df.columns)\n",
    "x_features =[]\n",
    "for feature in df_cols:\n",
    "    if feature not in removed_cols:\n",
    "        x_features.append(feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"objective\" : \"poisson\",\n",
    "    \"metric\" :\"rmse\",\n",
    "    'boosting_type' : 'gbdt',\n",
    "    \"force_row_wise\" : True,\n",
    "    \"learning_rate\" : 0.075,\n",
    "    #\"sub_feature\" : 0.8,\n",
    "    \"sub_row\" : 0.75,\n",
    "    \"bagging_freq\" : 1,\n",
    "    \"lambda_l2\" : 1.5,\n",
    "    \"lambda_l1\" : 0.5,\n",
    "    \"nthread\" : 5,\n",
    "    \"metric\": \"rmse\",\n",
    "    'verbosity': -1,\n",
    "    'num_iterations' : 2000,\n",
    "    'num_leaves': 200,\n",
    "    \"min_data_in_leaf\": 200,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['wday','event_name_1', 'event_type_1', 'event_name_2', 'event_type_2'] + \\\n",
    "    ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_train_data = lgb.Dataset(df[x_features], label = df['sales'])\n",
    "# lgbm_valid_data = lgb.Dataset(tst[x_features], label = tst['sales'])\n",
    "\n",
    "# del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = lgb.train(param, train_set=lgbm_train_data, valid_sets=lgbm_valid_data, categorical_feature=categorical_cols, \n",
    "#             verbose_eval=100, early_stopping_rounds=120)\n",
    "# model = lgb.train(param, train_set=lgbm_train_data, categorical_feature=categorical_cols)\n",
    "model = lgb.train(param, train_set=lgbm_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/hshan/Kaggle/model_WI_3.sav']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = '/Users/hshan/Kaggle/model_WI_3.sav'\n",
    "joblib.dump(model, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del lgbm_train_data, lgbm_valid_data, tr, tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = joblib.load('/Users/hshan/Kaggle/model_no_val.sav')\n",
    "df = load_df(train_last, pred_period, is_train=False, store='WI_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-04-25 00:00:00\n",
      "2016-04-26 00:00:00\n",
      "2016-04-27 00:00:00\n",
      "2016-04-28 00:00:00\n",
      "2016-04-29 00:00:00\n",
      "2016-04-30 00:00:00\n",
      "2016-05-01 00:00:00\n",
      "2016-05-02 00:00:00\n",
      "2016-05-03 00:00:00\n",
      "2016-05-04 00:00:00\n",
      "2016-05-05 00:00:00\n",
      "2016-05-06 00:00:00\n",
      "2016-05-07 00:00:00\n",
      "2016-05-08 00:00:00\n",
      "2016-05-09 00:00:00\n",
      "2016-05-10 00:00:00\n",
      "2016-05-11 00:00:00\n",
      "2016-05-12 00:00:00\n",
      "2016-05-13 00:00:00\n",
      "2016-05-14 00:00:00\n",
      "2016-05-15 00:00:00\n",
      "2016-05-16 00:00:00\n",
      "2016-05-17 00:00:00\n",
      "2016-05-18 00:00:00\n",
      "2016-05-19 00:00:00\n",
      "2016-05-20 00:00:00\n",
      "2016-05-21 00:00:00\n",
      "2016-05-22 00:00:00\n",
      "2016-05-23 00:00:00\n",
      "2016-05-24 00:00:00\n",
      "2016-05-25 00:00:00\n",
      "2016-05-26 00:00:00\n",
      "2016-05-27 00:00:00\n",
      "2016-05-28 00:00:00\n",
      "2016-05-29 00:00:00\n",
      "2016-05-30 00:00:00\n",
      "2016-05-31 00:00:00\n",
      "2016-06-01 00:00:00\n",
      "2016-06-02 00:00:00\n",
      "2016-06-03 00:00:00\n",
      "2016-06-04 00:00:00\n",
      "2016-06-05 00:00:00\n",
      "2016-06-06 00:00:00\n",
      "2016-06-07 00:00:00\n",
      "2016-06-08 00:00:00\n",
      "2016-06-09 00:00:00\n",
      "2016-06-10 00:00:00\n",
      "2016-06-11 00:00:00\n",
      "2016-06-12 00:00:00\n",
      "2016-06-13 00:00:00\n",
      "2016-06-14 00:00:00\n",
      "2016-06-15 00:00:00\n",
      "2016-06-16 00:00:00\n",
      "2016-06-17 00:00:00\n",
      "2016-06-18 00:00:00\n",
      "2016-06-19 00:00:00\n"
     ]
    }
   ],
   "source": [
    "first_day = datetime(2016, 4, 25)\n",
    "for delta in range(0, pred_period):\n",
    "    predict_day = first_day + timedelta(days = delta)\n",
    "    predict_df = df.loc[(pd.to_datetime(df.date) <= predict_day) & (pd.to_datetime(df.date) >= predict_day - timedelta(days=57))]\n",
    "    \n",
    "    predict_df = lag_features(predict_df)\n",
    "    \n",
    "    predict_df = predict_df.loc[pd.to_datetime(predict_df.date) == predict_day]\n",
    "    predict_df= predict_df[x_features]\n",
    "    result = model.predict(predict_df)\n",
    "    \n",
    "    df.loc[pd.to_datetime(df.date)==predict_day,'sales'] = result\n",
    "    del predict_df\n",
    "    print(predict_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(result_df):\n",
    "    '''result_df is the resulted dataframe from for looping in predicting, \n",
    "    it includes 56 samples before the first day of the prediction d_1914'''\n",
    "    sub_df = df.loc[pd.to_datetime(df.date)>= first_day,['id','d','sales']]\n",
    "    val_df = sub_df.loc[(pd.to_datetime(df.date)>= first_day)& (pd.to_datetime(df.date)< first_day+timedelta(days=28))]\n",
    "    eval_df = sub_df.loc[pd.to_datetime(df.date)>= (first_day+timedelta(days=28))]\n",
    "    \n",
    "    col_v= list(val_df['d'].unique())\n",
    "    col_e= list(eval_df['d'].unique())\n",
    "    \n",
    "    f_cols =[]\n",
    "    for i in range(1, 29):\n",
    "        f_col = f'F{i}'\n",
    "        f_cols.append(f_col)\n",
    "    \n",
    "    val_df = val_df.set_index([\"id\", \"d\" ]).unstack()['sales'][col_v].reset_index()\n",
    "    eval_df = eval_df.set_index([\"id\", \"d\" ]).unstack()['sales'][col_e].reset_index()\n",
    "    \n",
    "    val_df.columns=['id'] + f_cols\n",
    "    eval_df.columns=['id'] + f_cols\n",
    "    \n",
    "    for i in range(0,len(eval_df)):\n",
    "        eval_df['id'][i] = eval_df['id'][i].replace('validation','evaluation')\n",
    "        final_sub = pd.concat([val_df, eval_df])\n",
    "        \n",
    "    return (final_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_1=submission(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_2=submission(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_3=submission(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_4=submission(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_1=submission(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_2=submission(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_3=submission(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi_1=submission(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi_2=submission(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi_3=submission(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60980, 29)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_sub = [ca_1,ca_2,ca_3,ca_4,tx_1,tx_2,tx_3,wi_1,wi_2,wi_3]\n",
    "sub = pd.concat(store_sub)\n",
    "sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('/Users/hshan/Downloads/M5/model_sub6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'tweedie',\n",
    "                    'tweedie_variance_power': 1.1,\n",
    "                    'metric': 'rmse',\n",
    "                    'subsample': 0.5,\n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.03,\n",
    "                    'num_leaves': 2**11-1,\n",
    "                    'min_data_in_leaf': 2**12-1,\n",
    "                    'feature_fraction': 0.5,\n",
    "                    'max_bin': 100,\n",
    "                    'n_estimators': 1400,\n",
    "                    'boost_from_average': False,\n",
    "                    'verbose': -1,\n",
    "                } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model sub\n",
    "param = {\n",
    "    \"objective\" : \"poisson\",\n",
    "    \"metric\" :\"rmse\",\n",
    "    'boosting_type' : 'gbdt',\n",
    "    \"force_row_wise\" : True,\n",
    "    \"learning_rate\" : 0.075,\n",
    "    #\"sub_feature\" : 0.8,\n",
    "    \"sub_row\" : 0.75,\n",
    "    \"bagging_freq\" : 1,\n",
    "    \"lambda_l2\" : 1.5,\n",
    "    \"lambda_l1\" : 0.5,\n",
    "    \"nthread\" : 5,\n",
    "    \"metric\": \"rmse\",\n",
    "    'verbosity': -1,\n",
    "    'num_iterations' : 2000,\n",
    "    'num_leaves': 200,\n",
    "    \"min_data_in_leaf\": 200,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('/Users/hshan/Downloads/M5/model_sub6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.loc[submission['id'].str.contains('CA_1'), 'F1'] *= 1.009\n",
    "submission.loc[submission['id'].str.contains('CA_2'), 'F1'] *= 1.02\n",
    "submission.loc[submission['id'].str.contains('CA_3'), 'F1'] *= 1.009\n",
    "submission.loc[submission['id'].str.contains('CA_4'), 'F1'] *= 1.011\n",
    "\n",
    "submission.loc[submission['id'].str.contains('TX_1'), 'F1'] *= 1.01\n",
    "submission.loc[submission['id'].str.contains('TX_2'), 'F1'] *= 1.011\n",
    "submission.loc[submission['id'].str.contains('TX_3'), 'F1'] *= 1.011\n",
    "\n",
    "submission.loc[submission['id'].str.contains('WI_1'), 'F1'] *= 1.015\n",
    "submission.loc[submission['id'].str.contains('WI_2'), 'F1'] *= 1.02\n",
    "submission.loc[submission['id'].str.contains('WI_3'), 'F1'] *= 1.015\n",
    "\n",
    "\n",
    "submission.loc[submission['id'].str.contains('CA_1'), 'F2'] *= 0.994\n",
    "submission.loc[submission['id'].str.contains('CA_2'), 'F2'] *= 0.998\n",
    "submission.loc[submission['id'].str.contains('CA_3'), 'F2'] *= 0.994\n",
    "submission.loc[submission['id'].str.contains('CA_4'), 'F2'] *= 0.996\n",
    "\n",
    "submission.loc[submission['id'].str.contains('TX_1'), 'F2'] *= 0.995\n",
    "submission.loc[submission['id'].str.contains('TX_2'), 'F2'] *= 0.996\n",
    "submission.loc[submission['id'].str.contains('TX_3'), 'F2'] *= 0.996\n",
    "\n",
    "submission.loc[submission['id'].str.contains('WI_1'), 'F2'] *= 0.998\n",
    "submission.loc[submission['id'].str.contains('WI_2'), 'F2'] *= 1\n",
    "submission.loc[submission['id'].str.contains('WI_3'), 'F2'] *= 0.998\n",
    "\n",
    "\n",
    "submission.loc[submission['id'].str.contains('CA_1'), 'F3'] *= 0.994\n",
    "submission.loc[submission['id'].str.contains('CA_2'), 'F3'] *= 0.998\n",
    "submission.loc[submission['id'].str.contains('CA_3'), 'F3'] *= 0.994\n",
    "submission.loc[submission['id'].str.contains('CA_4'), 'F3'] *= 0.996\n",
    "\n",
    "submission.loc[submission['id'].str.contains('TX_1'), 'F3'] *= 0.995\n",
    "submission.loc[submission['id'].str.contains('TX_2'), 'F3'] *= 0.996\n",
    "submission.loc[submission['id'].str.contains('TX_3'), 'F3'] *= 0.996\n",
    "\n",
    "submission.loc[submission['id'].str.contains('WI_1'), 'F3'] *= 1\n",
    "submission.loc[submission['id'].str.contains('WI_2'), 'F3'] *= 1.005\n",
    "submission.loc[submission['id'].str.contains('WI_3'), 'F3'] *= 1\n",
    "\n",
    "\n",
    "submission.loc[submission['id'].str.contains('CA_1'), 'F4'] *= 0.98\n",
    "submission.loc[submission['id'].str.contains('CA_2'), 'F4'] *= 1.02\n",
    "submission.loc[submission['id'].str.contains('CA_3'), 'F4'] *= 0.98\n",
    "submission.loc[submission['id'].str.contains('CA_4'), 'F4'] *= 1\n",
    "\n",
    "submission.loc[submission['id'].str.contains('TX_1'), 'F4'] *= 0.99\n",
    "submission.loc[submission['id'].str.contains('TX_2'), 'F4'] *= 1\n",
    "submission.loc[submission['id'].str.contains('TX_3'), 'F4'] *= 1\n",
    "\n",
    "submission.loc[submission['id'].str.contains('WI_1'), 'F4'] *= 1.00\n",
    "submission.loc[submission['id'].str.contains('WI_2'), 'F4'] *= 1.02\n",
    "submission.loc[submission['id'].str.contains('WI_3'), 'F4'] *= 1.00\n",
    "\n",
    "\n",
    "submission.loc[submission['id'].str.contains('CA_1'), 'F5'] *= 0.995\n",
    "submission.loc[submission['id'].str.contains('CA_2'), 'F5'] *= 1\n",
    "submission.loc[submission['id'].str.contains('CA_3'), 'F5'] *= 0.995\n",
    "submission.loc[submission['id'].str.contains('CA_4'), 'F5'] *= 0.998\n",
    "\n",
    "submission.loc[submission['id'].str.contains('TX_1'), 'F5'] *= 0.996\n",
    "submission.loc[submission['id'].str.contains('TX_2'), 'F5'] *= 0.998\n",
    "submission.loc[submission['id'].str.contains('TX_3'), 'F5'] *= 0.998\n",
    "\n",
    "submission.loc[submission['id'].str.contains('WI_1'), 'F5'] *= 1\n",
    "submission.loc[submission['id'].str.contains('WI_2'), 'F5'] *= 1.01\n",
    "submission.loc[submission['id'].str.contains('WI_3'), 'F5'] *= 1\n",
    "\n",
    "\n",
    "submission.loc[submission['id'].str.contains('CA_1'), 'F6'] *= 1.0002\n",
    "submission.loc[submission['id'].str.contains('CA_2'), 'F6'] *= 1.001\n",
    "submission.loc[submission['id'].str.contains('CA_3'), 'F6'] *= 1.0002\n",
    "submission.loc[submission['id'].str.contains('CA_4'), 'F6'] *= 1.001\n",
    "\n",
    "submission.loc[submission['id'].str.contains('TX_1'), 'F6'] *= 1.000\n",
    "submission.loc[submission['id'].str.contains('TX_2'), 'F6'] *= 1.001\n",
    "submission.loc[submission['id'].str.contains('TX_3'), 'F6'] *= 1.001\n",
    "\n",
    "submission.loc[submission['id'].str.contains('WI_1'), 'F6'] *= 1.001\n",
    "submission.loc[submission['id'].str.contains('WI_2'), 'F6'] *= 1.01\n",
    "submission.loc[submission['id'].str.contains('WI_3'), 'F6'] *= 1.001\n",
    "\n",
    "\n",
    "submission.loc[submission['id'].str.contains('CA_1'), 'F7'] *= 0.994\n",
    "submission.loc[submission['id'].str.contains('CA_2'), 'F7'] *= 1\n",
    "submission.loc[submission['id'].str.contains('CA_3'), 'F7'] *= 0.994\n",
    "submission.loc[submission['id'].str.contains('CA_4'), 'F7'] *= 0.998\n",
    "\n",
    "submission.loc[submission['id'].str.contains('TX_1'), 'F7'] *= 0.995635\n",
    "submission.loc[submission['id'].str.contains('TX_2'), 'F7'] *= 0.998\n",
    "submission.loc[submission['id'].str.contains('TX_3'), 'F7'] *= 0.998\n",
    "\n",
    "submission.loc[submission['id'].str.contains('WI_1'), 'F7'] *= 1\n",
    "submission.loc[submission['id'].str.contains('WI_2'), 'F7'] *= 1.01\n",
    "submission.loc[submission['id'].str.contains('WI_3'), 'F7'] *= 1\n",
    "\n",
    "\n",
    "submission.loc[submission['id'].str.contains('CA_1'), 'F8'] *= 0.996\n",
    "submission.loc[submission['id'].str.contains('CA_2'), 'F8'] *= 1\n",
    "submission.loc[submission['id'].str.contains('CA_3'), 'F8'] *= 0.996\n",
    "submission.loc[submission['id'].str.contains('CA_4'), 'F8'] *= 0.998\n",
    "\n",
    "submission.loc[submission['id'].str.contains('TX_1'), 'F8'] *= 0.9988\n",
    "submission.loc[submission['id'].str.contains('TX_2'), 'F8'] *= 1\n",
    "submission.loc[submission['id'].str.contains('TX_3'), 'F8'] *= 1\n",
    "\n",
    "submission.loc[submission['id'].str.contains('WI_1'), 'F8'] *= 1\n",
    "submission.loc[submission['id'].str.contains('WI_2'), 'F8'] *= 1.01\n",
    "submission.loc[submission['id'].str.contains('WI_3'), 'F8'] *= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114355.66201856881\n",
      "119442.08301150176\n",
      "90030.69711273958\n",
      "83256.40634146568\n",
      "83523.20867694943\n",
      "79813.90486883943\n",
      "91404.69892056477\n",
      "106857.919951501\n",
      "102881.49510454017\n"
     ]
    }
   ],
   "source": [
    "for i in range(9,20):\n",
    "    if i!=11:\n",
    "        #submission['F'+str(i)] *= 1.01 \n",
    "        submission.loc[submission['id'].str.contains('CA_1'), 'F'+str(i)] *= 1\n",
    "        submission.loc[submission['id'].str.contains('CA_2'), 'F'+str(i)] *= 1.015\n",
    "        submission.loc[submission['id'].str.contains('CA_3'), 'F'+str(i)] *= 1\n",
    "        submission.loc[submission['id'].str.contains('CA_4'), 'F'+str(i)] *= 1.013\n",
    "\n",
    "        submission.loc[submission['id'].str.contains('TX_1'), 'F'+str(i)] *= 1\n",
    "        submission.loc[submission['id'].str.contains('TX_2'), 'F'+str(i)] *= 1.013\n",
    "        submission.loc[submission['id'].str.contains('TX_3'), 'F'+str(i)] *= 1.013\n",
    "\n",
    "        submission.loc[submission['id'].str.contains('WI_1'), 'F'+str(i)] *= 1.015\n",
    "        submission.loc[submission['id'].str.contains('WI_2'), 'F'+str(i)] *= 1.02\n",
    "        submission.loc[submission['id'].str.contains('WI_3'), 'F'+str(i)] *= 1.015\n",
    "        \n",
    "for i in range(20,29):\n",
    "    #submission['F'+str(i)] *= 1.02\n",
    "    submission.loc[submission['id'].str.contains('CA_1'), 'F'+str(i)] *= 1.015\n",
    "    submission.loc[submission['id'].str.contains('CA_2'), 'F'+str(i)] *= 1.03\n",
    "    submission.loc[submission['id'].str.contains('CA_3'), 'F'+str(i)] *= 1.015\n",
    "    submission.loc[submission['id'].str.contains('CA_4'), 'F'+str(i)] *= 1.025\n",
    "\n",
    "    submission.loc[submission['id'].str.contains('TX_1'), 'F'+str(i)] *= 1.02\n",
    "    submission.loc[submission['id'].str.contains('TX_2'), 'F'+str(i)] *= 1.025\n",
    "    submission.loc[submission['id'].str.contains('TX_3'), 'F'+str(i)] *= 1.025\n",
    "\n",
    "    submission.loc[submission['id'].str.contains('WI_1'), 'F'+str(i)] *= 1.03\n",
    "    submission.loc[submission['id'].str.contains('WI_2'), 'F'+str(i)] *= 1.04\n",
    "    submission.loc[submission['id'].str.contains('WI_3'), 'F'+str(i)] *= 1.03\n",
    "    print(submission['F'+str(i)].sum())\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('/Users/hshan/Downloads/M5/model_sub66.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
