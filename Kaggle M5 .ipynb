{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hshan/opt/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from  datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plot \n",
    "import random\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read_load\n",
    "melt_merge\n",
    "split_data\n",
    "missing val\n",
    "encoding cat var\n",
    "lag by 7, 28\n",
    "selected col\n",
    "/Users/hshan/Downloads/M5/sell_prices.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df():\n",
    "    \n",
    "    cal_cat_cols = ['weekday', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    sales_cat_cols = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "    price_cat_cols = ['store_id', 'item_id']\n",
    "    \n",
    "    calendars = pd.read_csv('/Users/hshan/Downloads/M5/calendar.csv')\n",
    "    sales = pd.read_csv('/Users/hshan/Downloads/M5/sales_train_validation.csv')\n",
    "    prices = pd.read_csv('/Users/hshan/Downloads/M5/sell_prices.csv')\n",
    "    \n",
    "    ind_var = ['id'] + sales_cat_cols\n",
    "    val_var = [col for col in sales.columns if col.startswith('d_')]\n",
    "    df = pd.melt(sales, id_vars = ind_var, value_vars = val_var, var_name = 'd', value_name='sales')\n",
    "    df = df.merge(calendars, on = 'd', copy = False)\n",
    "    df = df.merge(prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n",
    "    \n",
    "    cat_cols = cal_cat_cols + sales_cat_cols\n",
    "    label_encoder = LabelEncoder()\n",
    "    for col in cat_cols:\n",
    "        df[col] = df[col].fillna('').astype('category')\n",
    "        df[col] = label_encoder.fit_transform(df[col])\n",
    "\n",
    "    unused_cols = ['date','wm_yr_wk', 'weekday']\n",
    "    \n",
    "    df.drop(unused_cols, inplace = True, axis = 1)\n",
    "    \n",
    "    return (df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_features(df):\n",
    "    '''max lag should not be exceeding 57 in this case'''\n",
    "    num = [1,7,28]\n",
    "    lags = num\n",
    "    windows = num\n",
    "    lag_cols = [f'lag_{lag}' for lag in lags]\n",
    "    \n",
    "\n",
    "    for lag, lag_col in zip(lags, lag_cols):\n",
    "        df[lag_col] = df[['id','sales']].groupby('id')['sales'].shift(lag)\n",
    "        \n",
    "    for window in windows:\n",
    "        for lag, lag_col in zip(lags, lag_cols):\n",
    "            mean_col = f'mean_{lag}_{window}'\n",
    "            df[mean_col] = df[['id',lag_col]].groupby('id')[lag_col].transform(lambda x: x.rolling(window).mean())\n",
    "    \n",
    "\n",
    "    return (df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['weekday', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2'] + \\\n",
    "    ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "removed_cols = ['id', 'date', 'sales', 'd', 'wm_yr_wk', 'weekday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(x_features_df, n_components):\n",
    "    pca = PCA(n_components)\n",
    "    pca_cols = []\n",
    "    for i in range(n_components):\n",
    "        pca_col = f'pc_{i+1}'\n",
    "        pca_cols.append(pca_col)\n",
    "    p_components = pca.fit_transform(x_features_df)\n",
    "    pca_features_df = pd.DataFrame(data = p_components, columns = pca_cols)\n",
    "    \n",
    "    return(pca_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_period = 28\n",
    "train_start = 1\n",
    "train_last = 1913\n",
    "test_start = train_last + 1\n",
    "test_last  = train_last + pred_period\n",
    "pred_start = test_last + 1\n",
    "pred_last = test_last + pred_period\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_df()\n",
    "df = lag_features(df)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols = list(df.columns)\n",
    "x_features =[]\n",
    "for feature in df_cols:\n",
    "    if feature not in removed_cols:\n",
    "        x_features.append(feature)\n",
    "y_target = 'sales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = int(0.65*(len(df_cols)-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_features_df = pca(df[x_features], n_components)\n",
    "y_target = df['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xVal, yTrain, yVal = train_test_split(pca_features_df, y_target, test_size = 0.3, random_state = seed)\n",
    "\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pca_features_df, y_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Parameter values for parameter (learning_rate) need to be a sequence(but not a string) or np.ndarray.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-138a6a20b553>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     }\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlgbm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgridParams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, estimator, param_grid, scoring, n_jobs, iid, refit, cv, verbose, pre_dispatch, error_score, return_train_score)\u001b[0m\n\u001b[1;32m   1143\u001b[0m             return_train_score=return_train_score)\n\u001b[1;32m   1144\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0m_check_param_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_check_param_grid\u001b[0;34m(param_grid)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 raise ValueError(\"Parameter values for parameter ({0}) need \"\n\u001b[1;32m    370\u001b[0m                                  \u001b[0;34m\"to be a sequence(but not a string) or\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                                  \" np.ndarray.\".format(name))\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Parameter values for parameter (learning_rate) need to be a sequence(but not a string) or np.ndarray."
     ]
    }
   ],
   "source": [
    "lgbm_model = lgb.LGBMModel(\n",
    "    objective = 'poisson',\n",
    "    boosting_type = 'gbdt',\n",
    "    metric ='rmse',\n",
    "    force_row_wise = False,\n",
    "    learning_rate = 0.075,\n",
    "    lambda_l2 = 0.1,\n",
    "    verbosity = 1,\n",
    "    num_iterations = 500,\n",
    "    num_leaves = 100,\n",
    "    boost_from_average = False,\n",
    "    random_state = seed,\n",
    "    n_estimators = 300,\n",
    "    )\n",
    "\n",
    "gridParams = {\n",
    "    'learning_rate': 0.01,\n",
    "    'lambda_l2' : 0.2,\n",
    "    'num_leaves': 200,\n",
    "    'num_iterations' : 200,\n",
    "    'n_estimators': [100]\n",
    "    }\n",
    "\n",
    "grid = GridSearchCV(lgbm_model, gridParams, cv=3, n_jobs=-1, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'objective' : 'poisson',\n",
    "    'boosting_type' : 'gbdt',\n",
    "    'metric' : 'rmse',\n",
    "    'force_row_wise' : True,\n",
    "    'learning_rate' : 0.075,\n",
    "    'lambda_l2' : 0.1,\n",
    "    'verbosity' : 1,\n",
    "    'num_iterations' : 1500,\n",
    "    'num_leaves' : 150,\n",
    "    'boost_from_average' : False,\n",
    "    'random_state' : seed,\n",
    "    'n_estimators' : 300,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"objective\" : \"poisson\",\n",
    "    \"metric\" :\"rmse\",\n",
    "    'boosting_type' : 'gbdt',\n",
    "    \"force_row_wise\" : True,\n",
    "    \"learning_rate\" : 0.075,\n",
    "    #\"sub_feature\" : 0.8,\n",
    "    \"sub_row\" : 0.75,\n",
    "    \"bagging_freq\" : 1,\n",
    "    \"lambda_l2\" : 0.1,\n",
    "    \"nthread\" : 5,\n",
    "    \"metric\": \"rmse\",\n",
    "    'verbosity': 1,\n",
    "    'num_iterations' : 3000,\n",
    "    'num_leaves': 250,\n",
    "    \"min_data_in_leaf\": 200,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_train_data = lgb.Dataset(xTrain, label = yTrain)\n",
    "lgbm_valid_data = lgb.Dataset(xVal, label = yVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del xTrain, yTrain, xVal, yVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 40 rounds\n",
      "[100]\tvalid_0's rmse: 2.42532\n",
      "[200]\tvalid_0's rmse: 2.39154\n",
      "[300]\tvalid_0's rmse: 2.37301\n",
      "[400]\tvalid_0's rmse: 2.36242\n",
      "[500]\tvalid_0's rmse: 2.35494\n",
      "[600]\tvalid_0's rmse: 2.34935\n",
      "[700]\tvalid_0's rmse: 2.34614\n",
      "[800]\tvalid_0's rmse: 2.34344\n",
      "[900]\tvalid_0's rmse: 2.34099\n",
      "[1000]\tvalid_0's rmse: 2.33874\n",
      "[1100]\tvalid_0's rmse: 2.33776\n",
      "[1200]\tvalid_0's rmse: 2.33625\n",
      "[1300]\tvalid_0's rmse: 2.33498\n",
      "[1400]\tvalid_0's rmse: 2.33424\n",
      "[1500]\tvalid_0's rmse: 2.33339\n",
      "[1600]\tvalid_0's rmse: 2.33286\n",
      "[1700]\tvalid_0's rmse: 2.33241\n",
      "[1800]\tvalid_0's rmse: 2.33171\n",
      "[1900]\tvalid_0's rmse: 2.3312\n",
      "[2000]\tvalid_0's rmse: 2.33068\n",
      "[2100]\tvalid_0's rmse: 2.33008\n",
      "Early stopping, best iteration is:\n",
      "[2138]\tvalid_0's rmse: 2.32998\n"
     ]
    }
   ],
   "source": [
    "model = lgb.train(param, train_set=lgbm_train_data, valid_sets=lgbm_valid_data, verbose_eval=100,\n",
    "                  early_stopping_rounds=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(pca_features_df, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['learning_rate'] = grid.best_params_['learning_rate']\n",
    "params['num_leaves'] = grid.best_params_['num_leaves']\n",
    "params['lambda_l2'] = grid.best_params_['lambda_l2']\n",
    "params['num_iterations'] = grid.best_params_['num_iterations']\n",
    "params['subsample'] = grid.best_params_['subsample']\n",
    "params['n_estimators'] = grid.best_params_['n_estimators']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "del lgbm_train_data, lgbm_valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = load_df()\n",
    "pred_df = lag_features(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model.sav'\n",
    "joblib.dump(model, filename)\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = joblib.load(filename)\n",
    "result = loaded_model.score(X_test, Y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df['sales'].fillna(0, inplace=True)\n",
    "pred_df = pred_df.loc[test_start:pred_last]\n",
    "del pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_x_pca_df = pca(pred_df[x_features, n_components)\n",
    "\n",
    "pred_df['sales'] = model.predict(pred_x_pca_df)\n",
    "result = pred_df[['id', 'd', 'sales']]\n",
    "\n",
    "def submission_df():\n",
    "    df = load_data()\n",
    "    submission['id'] = df['id']\n",
    "    del df \n",
    "    submission = pd.DataFrame()\n",
    "    for i in range(pred_period):\n",
    "        submission_col = f'd_{i}'\n",
    "        submission[submission_col] = pd.Series()\n",
    "        \n",
    "    return (submission_df)\n",
    "\n",
    "df_unmelted = result.pivot(index='id', columns='d')\n",
    "df_unmelted = df_unmelted['Value'].reset_index()\n",
    "df_unmelted.columns.name = None\n",
    "\n",
    "submission_df.merge(df_unmelted, on = 'id')\n",
    "for i in range(pred_period):\n",
    "    col = f'F{i+1}'\n",
    "    ori = f'd_{i+1}'\n",
    "    submission_df.rename(columns = {ori:col})\n",
    "\n",
    "submission_df.to_csv('/Users/hshan/Downloads/M5/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(test.drop(['id'], axis=1))\n",
    "ids = test['id'].values\n",
    "\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state = 12)\n",
    "    \n",
    "del X, y; gc.collect();\n",
    "\n",
    "d_train = lgb.Dataset(X_train, label=y_train)\n",
    "d_valid = lgb.Dataset(X_valid, label=y_valid) \n",
    "\n",
    "watchlist = [d_train, d_valid]\n",
    "\n",
    "\n",
    "model = lgb.train(params, train_set=d_train, num_boost_round=1000, valid_sets=watchlist, early_stopping_rounds=50, verbose_eval=4)\n",
    "\n",
    "p_test = model.predict(X_test)\n",
    "\n",
    "subm = pd.DataFrame()\n",
    "subm['id'] = ids\n",
    "subm['target'] = p_test\n",
    "submName = strftime(\"%Y%m%d%H%M%S\", gmtime()) + '_submission.csv.gz'\n",
    "subm.to_csv(submName, compression = 'gzip', index=False, float_format = '%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_dtype={\n",
    "    \"event_name_1\": \"category\", \n",
    "    \"event_name_2\": \"category\", \n",
    "    \"event_type_1\": \"category\", \n",
    "    \"event_type_2\": \"category\", \n",
    "    \"weekday\": \"category\", \n",
    "    'wm_yr_wk': 'int16', \n",
    "    \"wday\": \"int16\",\n",
    "    \"month\": \"int16\", \n",
    "    \"year\": \"int16\", \n",
    "    \"snap_CA\": \"float32\", \n",
    "    'snap_TX': 'float32', \n",
    "    'snap_WI': 'float32' }\n",
    "price_dtype = {\n",
    "    \"store_id\": \"category\", \n",
    "    \"item_id\": \"category\", \n",
    "    \"wm_yr_wk\": \"int16\",\n",
    "    \"sell_price\":\"float32\" }\n",
    "catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n",
    "numcols = [f\"d_{day}\" for day in range(1,29)]\n",
    "sales_dtype = {numcol:\"float32\" for numcol in numcols} \n",
    "sales_dtype.update({col: \"category\" for col in catcols if col != \"id\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
